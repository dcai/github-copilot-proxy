import type { ChatCompletionPayload, CompletionResponse } from "./helper";
import { ContentfulStatusCode } from "hono/utils/http-status";
import type { Message } from "./helper";
import { Hono } from "hono";
import type { Context } from "hono";
import {
  findSystemMessageContent,
  findUserMessageContent,
  getHeaders,
  hasImageInRequestBody,
  logger,
  makeReadableStream,
} from "./helper";

// Simulate Llama-style API for GitHub Copilot
// https://github.com/ollama/ollama/blob/main/docs/api.md

export type OllamaMessage = {
  content: string;
  role: "user" | "assistant" | "system";
};
export interface OllamaCompletionRequest {
  messages: OllamaMessage[];
  stream: boolean;
  model: string;
}

// Llama-style response shape
export interface OllamaCompletionResponse {
  model: string;
  created?: string;
  message?: OllamaMessage;
  messages?: OllamaMessage[];
  done: boolean;
  done_reason: "stop" | "length" | "content_filter" | "tool_use";
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

// Convert Llama request to Copilot chat payload
export function llamaToCopilotPayload(
  req: OllamaCompletionRequest,
): ChatCompletionPayload {
  return {
    model: "gpt-4.1", // default model
    messages: req.messages.map((msg) => {
      return {
        content: msg.content,
        role: msg.role,
      };
    }),
  };
}

// Convert Copilot response to Llama-style response
export function copilotToLlamaResponse(
  resp: CompletionResponse,
): OllamaCompletionResponse {
  const choice = resp.choices?.[0];
  const text = choice?.message?.content || "";
  // Simple token count approximation by splitting on whitespace
  const tokenCount = text.split(/\s+/).filter(Boolean).length;
  return {
    model: "llama",
    created: new Date().toISOString(),
    message: {
      content: text,
      "role": "assistant",
    },
    done_reason: "stop",
    done: true,
    "total_duration": 4883583458,
    "load_duration": 1334875,
    "prompt_eval_count": 26,
    "prompt_eval_duration": 342546000,
    "eval_count": 282,
    "eval_duration": 4535599000,
  };
}

export const llamaRoutes = new Hono();

// Llama-style completions proxy (no streaming)
llamaRoutes.post("/api/chat", async (c: Context) => {
  try {
    const llamaReq = (await c.req.json()) as OllamaCompletionRequest;
    // Map Llama request to Copilot payload
    const copilotPayload = llamaToCopilotPayload(llamaReq);
    const headers = await getHeaders();
    // Call Copilot backend
    const upstream = await fetch(
      "https://api.githubcopilot.com/chat/completions",
      { method: "POST", headers, body: JSON.stringify(copilotPayload) },
    );
    const text = await upstream.text();
    console.info(
      "ðŸ¤” ///// llama.ts @ LINE 105",
      JSON.stringify(JSON.parse(text), null, 1),
    );
    if (!upstream.ok) {
      return c.json(
        { error: { message: text, code: upstream.status } },
        upstream.status as unknown as ContentfulStatusCode,
      );
    }
    const copilotResp = JSON.parse(text) as CompletionResponse;
    const llamaResp = copilotToLlamaResponse(copilotResp);
    console.info(
      "ðŸ¤” ///// llama.ts @ LINE 97",
      JSON.stringify(llamaResp, null, 1),
    );
    return c.json(llamaResp);
  } catch (err: any) {
    logger.error(err);
    return c.json(
      { error: { message: String(err), code: 500 } },
      500 as unknown as ContentfulStatusCode,
    );
  }
});

llamaRoutes.get("/api/tags", async (c: Context) => {
  return c.json({
    models: [
      {
        "name": "llama",
        "model": "llama",
        "modified_at": "2023-12-07T09:32:18.757212583Z",
        "size": 3825819519,
        "digest":
          "sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8",
        "details": {
          "parent_model": "",
          "format": "gguf",
          "family": "llama",
          "families": ["llama"],
          "parameter_size": "671B",
          "quantization_level": "Q4_0",
        },
      },
    ],
  });
});
llamaRoutes.post("/api/pull", async (c: Context) => {
  return c.json({ status: "success", message: "Model pull initiated." });
});
llamaRoutes.post("/api/show", async (c: Context) => {
  return c.json({
    "modelfile":
      '# Modelfile generated by "ollama show"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llama2:latest\n\nFROM /Users/username/.ollama/models/blobs/sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8\nTEMPLATE """[INST] <<SYS>>\n{{ .System }}\n<</SYS>>\n\n{{ .Prompt }} [/INST]\n"""\nSYSTEM """You are a helpful, respectful and honest assistant."""\nPARAMETER stop "[INST]"\nPARAMETER stop "[/INST]"',
    "parameters":
      'temperature 0.8\nrepeat_penalty 1.1\ntop_k 40\ntop_p 0.9\nstop "[INST]"\nstop "[/INST]"',
    "template":
      "[INST] <<SYS>>\n{{ .System }}\n<</SYS>>\n\n{{ .Prompt }} [/INST]",
    "system": "You are a helpful, respectful and honest assistant.",
    "details": {
      "parent_model": "",
      "format": "gguf",
      "family": "llama",
      "families": ["llama"],
      "parameter_size": "7B",
      "quantization_level": "Q4_0",
    },
    "model_info": {
      "general.architecture": "llama",
      "general.file_type": 2,
      "general.parameter_count": 6738415616,
      "general.quantization_version": 2,
      "llama.attention.head_count": 32,
      "llama.attention.head_count_kv": 32,
      "llama.attention.layer_norm_rms_epsilon": 0.000001,
      "llama.block_count": 32,
      "llama.context_length": 4096,
      "llama.embedding_length": 4096,
      "llama.feed_forward_length": 11008,
      "llama.rope.dimension_count": 128,
      "llama.rope.freq_base": 10000,
      "llama.vocab_size": 32000,
      "tokenizer.ggml.model": "llama",
    },
    "capabilities": [
      "completion",
      "vision",
      "chat",
      "embeddings",
      "function-calling",
      "tool-use",
      "tools",
    ],
  });
});
